{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, RANSACRegressor, Ridge, Lasso\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data Arrays\n",
    "x = np.load('X_train_regression2.npy')\n",
    "y = np.load('y_train_regression2.npy')\n",
    "xx = np.load('X_test_regression2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression with cross validation (loo)\n",
    "loo = LeaveOneOut()\n",
    "predicted_values = []\n",
    "sse_values = []\n",
    "coef_values = []\n",
    "for train_index, test_index in loo.split(x):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    lregr = LinearRegression()\n",
    "    lregr.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = lregr.predict(x_test)\n",
    "    sse_values.append(np.mean((y_test - y_pred)**2))\n",
    "\n",
    "    coef_values.append(lregr.coef_)\n",
    "    predicted_values.append(y_pred.flatten())\n",
    "\n",
    "mean_coef = np.mean(coef_values, axis=0)\n",
    "mse = np.mean((y-predicted_values)**2)\n",
    "var_y = sum((y - np.mean(y))**2)/len(y)\n",
    "r_squared = 1 - mse/var_y\n",
    "\n",
    "print(mean_coef[0], f\"\\nmse is: {mse}\", f\"\\nr^2 is: {r_squared.flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,4):\n",
    "    counts, bins = np.histogram(x[:,i])\n",
    "    plt.hist(bins[:-1], bins, weights=counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Gaussian Mixer, with linear regression\n",
    "data = np.concatenate([x,y],axis=1)\n",
    "gm = GaussianMixture(n_components=2, covariance_type='full', init_params='kmeans', random_state=69).fit(data)\n",
    "mask = gm.predict(data)\n",
    "cluster_1 = np.where(mask == 1)[0]\n",
    "cluster_2 = np.where(mask == 0)[0]\n",
    "print(f\"The cluster sizes are {len(cluster_1)} / {len(cluster_2)} respectively.\")\n",
    "\n",
    "x_cluster1, y_cluster1 = x[cluster_1], y[cluster_1]\n",
    "x_cluster2, y_cluster2 = x[cluster_2], y[cluster_2]\n",
    "\n",
    "print(\"===== Model 1 =====\")\n",
    "kf = KFold(n_splits=15, shuffle=True, random_state=0)\n",
    "predicted_values = []\n",
    "sse_values = []\n",
    "coef_values = []\n",
    "for train_index, test_index in kf.split(x_cluster1):\n",
    "    x_train, x_test = x_cluster1[train_index], x_cluster1[test_index]\n",
    "    y_train, y_test = y_cluster1[train_index], y_cluster1[test_index]\n",
    "\n",
    "    lregr = LinearRegression()\n",
    "    lregr.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = lregr.predict(x_test)\n",
    "    sse_values.append(np.mean((y_test - y_pred)**2))\n",
    "\n",
    "    coef_values.append(lregr.coef_)\n",
    "    predicted_values.append(y_pred.flatten())\n",
    "    \n",
    "sse = np.mean(sse_values)\n",
    "ss_tot = sum((y_cluster1 - np.mean(y_cluster1))**2)/len(y_cluster1)\n",
    "r_squared = 1 - sse/ss_tot\n",
    "\n",
    "mean_coef_1 = np.mean(coef_values, axis=0)\n",
    "print(\"Mean coefs\", mean_coef_1)\n",
    "print(\"mse:\",sse, \"\\nss_tot:\",ss_tot, \"\\nr_squared:\",r_squared)\n",
    "\n",
    "\n",
    "print(\"===== Model 2 =====\")\n",
    "kf2 = KFold(n_splits=15, shuffle=True, random_state=0)\n",
    "predicted_values = []\n",
    "sse_values = []\n",
    "coef_values = []\n",
    "\n",
    "for train_index, test_index in kf2.split(x_cluster2):\n",
    "    x_train, x_test = x_cluster2[train_index], x_cluster2[test_index]\n",
    "    y_train, y_test = y_cluster2[train_index], y_cluster2[test_index]\n",
    "\n",
    "    lregr = LinearRegression()\n",
    "    lregr.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = lregr.predict(x_test)\n",
    "    sse_values.append(np.mean((y_test - y_pred)**2))\n",
    "\n",
    "    coef_values.append(lregr.coef_)\n",
    "    predicted_values.append(y_pred.flatten())\n",
    "\n",
    "sse = np.mean(sse_values)\n",
    "ss_tot = sum((y_cluster2 - np.mean(y_cluster2))**2)/len(y_cluster2)\n",
    "r_squared = 1 - sse/ss_tot\n",
    "\n",
    "mean_coef_2 = np.mean(coef_values, axis=0)\n",
    "print(\"Mean coefs\", mean_coef_2)\n",
    "print(\"mse:\",sse, \"\\nss_tot:\",ss_tot, \"\\nr_squared:\",r_squared)\n",
    "\n",
    "# Generating output array\n",
    "\n",
    "\n",
    "lregr_1 = LinearRegression()\n",
    "lregr_1.fit(x_cluster1, y_cluster1)\n",
    "y_pred_1 = lregr_1.predict(xx)\n",
    "lregr_2 = LinearRegression()\n",
    "lregr_2.fit(x_cluster2, y_cluster2)\n",
    "y_pred_2 = lregr_2.predict(xx)\n",
    "y_out = np.concatenate([y_pred_1,y_pred_2], axis=1)\n",
    "np.save('Y_test_regression2.npy', y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,4):\n",
    "    counts, bins = np.histogram(x_cluster1[:,i])\n",
    "    plt.hist(bins[:-1], bins, weights=counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,4):\n",
    "    counts, bins = np.histogram(x_cluster2[:,i])\n",
    "    plt.hist(bins[:-1], bins, weights=counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Gaussian Mixer with Ridge\n",
    "data = np.concatenate([x,y],axis=1)\n",
    "gm = GaussianMixture(n_components=2, covariance_type='full', init_params='kmeans', random_state=42).fit(data)\n",
    "mask = gm.predict(data)\n",
    "cluster_1 = np.where(mask == 1)[0]\n",
    "cluster_2 = np.where(mask == 0)[0]\n",
    "print(f\"The cluster sizes are {len(cluster_1)} / {len(cluster_2)} respectively.\")\n",
    "\n",
    "x_cluster1, y_cluster1 = x[cluster_1], y[cluster_1]\n",
    "x_cluster2, y_cluster2 = x[cluster_2], y[cluster_2]\n",
    "\n",
    "print(\"===== Model 1 =====\")\n",
    "kf = KFold(n_splits=15, shuffle=True, random_state=47)\n",
    "coef_values = []\n",
    "mse_values = []\n",
    "mean_coef = []\n",
    "alpha_min = 0\n",
    "alphas = np.logspace(-4,-2,5000)\n",
    "for alpha in alphas:\n",
    "    sse_values = []\n",
    "    coef_values = []\n",
    "    for train_index, test_index in kf.split(x_cluster1):\n",
    "        x_train, x_test = x_cluster1[train_index], x_cluster1[test_index]\n",
    "        y_train, y_test = y_cluster1[train_index], y_cluster1[test_index]\n",
    "\n",
    "        ridge = Ridge(alpha=alpha, random_state=0)\n",
    "        ridge.fit(x_train, y_train)\n",
    "\n",
    "        y_pred = ridge.predict(x_test)\n",
    "        sse_fold = np.sum((y_test - y_pred)**2)\n",
    "        sse_values.append(sse_fold)\n",
    "        coef_values.append(ridge.coef_)\n",
    "    mse_values.append(np.mean(sse_values))\n",
    "    mean_coef.append(np.mean(coef_values, axis=0))\n",
    "    \n",
    "alpha_min = alphas[np.array(mse_values).argmin()]\n",
    "ss_tot = sum((y_cluster1 - np.mean(y_cluster1))**2)/len(y_cluster1)\n",
    "r_squared = 1 - np.min(mse_values)/ss_tot\n",
    "print(\"Mean coefs\", mean_coef[np.where(alphas == alpha_min)[0][0]])\n",
    "print(\"best alpha:\",alpha_min,\"mse:\",np.min(mse_values), \"\\nss_tot:\",ss_tot, \"\\nr_squared:\",r_squared.flatten())\n",
    "\n",
    "print(\"===== Model 2 =====\")\n",
    "kf2 = KFold(n_splits=15, shuffle=True, random_state=49)\n",
    "coef_values = []\n",
    "mse_values = []\n",
    "mean_coef = []\n",
    "alpha_min = 0\n",
    "alphas = np.logspace(-1,0,5000)\n",
    "for alpha in alphas:\n",
    "    sse_values = []\n",
    "    coef_values = []\n",
    "    for train_index, test_index in kf2.split(x_cluster2):\n",
    "        x_train, x_test = x_cluster2[train_index], x_cluster2[test_index]\n",
    "        y_train, y_test = y_cluster2[train_index], y_cluster2[test_index]\n",
    "\n",
    "        ridge = Ridge(alpha=alpha, random_state=0)\n",
    "        ridge.fit(x_train, y_train)\n",
    "\n",
    "        y_pred = ridge.predict(x_test)\n",
    "        sse_fold = np.sum((y_test - y_pred)**2)\n",
    "        sse_values.append(sse_fold)\n",
    "        coef_values.append(ridge.coef_)\n",
    "    mse_values.append(np.mean(sse_values))\n",
    "    mean_coef.append(np.mean(coef_values, axis=0))\n",
    "    \n",
    "alpha_min = alphas[np.array(mse_values).argmin()]\n",
    "ss_tot = sum((y_cluster2 - np.mean(y_cluster2))**2)/len(y_cluster2)\n",
    "r_squared = 1 - np.min(mse_values)/ss_tot\n",
    "print(\"Mean coefs\", mean_coef[np.where(alphas == alpha_min)[0][0]])\n",
    "print(\"best alpha:\",alpha_min,\"mse:\",np.min(mse_values), \"\\nss_tot:\",ss_tot, \"\\nr_squared:\",r_squared.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Gaussian Mixer with Lasso\n",
    "data = np.concatenate([x,y],axis=1)\n",
    "gm = GaussianMixture(n_components=2, covariance_type='full', init_params='kmeans', random_state=42).fit(data)\n",
    "mask = gm.predict(data)\n",
    "cluster_1 = np.where(mask == 1)[0]\n",
    "cluster_2 = np.where(mask == 0)[0]\n",
    "print(f\"The cluster sizes are {len(cluster_1)} / {len(cluster_2)} respectively.\")\n",
    "\n",
    "x_cluster1, y_cluster1 = x[cluster_1], y[cluster_1]\n",
    "x_cluster2, y_cluster2 = x[cluster_2], y[cluster_2]\n",
    "\n",
    "print(\"===== Model 1 =====\")\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=47)\n",
    "coef_values = []\n",
    "mse_values = []\n",
    "mean_coef = []\n",
    "alpha_min = 0\n",
    "alphas = np.logspace(-2,2,10_000)\n",
    "for alpha in alphas:\n",
    "    sse_values = []\n",
    "    coef_values = []\n",
    "    for train_index, test_index in kf.split(x_cluster1):\n",
    "        x_train, x_test = x_cluster1[train_index], x_cluster1[test_index]\n",
    "        y_train, y_test = y_cluster1[train_index], y_cluster1[test_index]\n",
    "\n",
    "        ridge = Lasso(alpha=alpha, random_state=0)\n",
    "        ridge.fit(x_train, y_train)\n",
    "\n",
    "        y_pred = ridge.predict(x_test)\n",
    "        sse_fold = np.sum((y_test - y_pred)**2)\n",
    "        sse_values.append(sse_fold)\n",
    "        coef_values.append(ridge.coef_)\n",
    "    mse_values.append(np.mean(sse_values))\n",
    "    mean_coef.append(np.mean(coef_values, axis=0))\n",
    "    \n",
    "alpha_min = alphas[np.array(mse_values).argmin()]\n",
    "ss_tot = sum((y_cluster1 - np.mean(y_cluster1))**2)/len(y_cluster1)\n",
    "r_squared = 1 - np.min(mse_values)/ss_tot\n",
    "print(\"Mean coefs\", mean_coef[np.where(alphas == alpha_min)[0][0]])\n",
    "print(\"best alpha:\",alpha_min,\"sse:\",np.min(mse_values), \"\\nss_tot:\",ss_tot, \"\\nr_squared:\",r_squared.flatten())\n",
    "\n",
    "print(\"===== Model 2 =====\")\n",
    "kf2 = KFold(n_splits=2, shuffle=True, random_state=49)\n",
    "coef_values = []\n",
    "mse_values = []\n",
    "mean_coef = []\n",
    "alpha_min = 0\n",
    "alphas = np.logspace(-2,2,10_000)\n",
    "for alpha in alphas:\n",
    "    sse_values = []\n",
    "    coef_values = []\n",
    "    for train_index, test_index in kf2.split(x_cluster2):\n",
    "        x_train, x_test = x_cluster2[train_index], x_cluster2[test_index]\n",
    "        y_train, y_test = y_cluster2[train_index], y_cluster2[test_index]\n",
    "\n",
    "        ridge = Lasso(alpha=alpha, random_state=0)\n",
    "        ridge.fit(x_train, y_train)\n",
    "\n",
    "        y_pred = ridge.predict(x_test)\n",
    "        sse_fold = np.sum((y_test - y_pred)**2)\n",
    "        sse_values.append(sse_fold)\n",
    "        coef_values.append(ridge.coef_)\n",
    "    mse_values.append(np.mean(sse_values))\n",
    "    mean_coef.append(np.mean(coef_values, axis=0))\n",
    "    \n",
    "alpha_min = alphas[np.array(mse_values).argmin()]\n",
    "ss_tot = sum((y_cluster2 - np.mean(y_cluster2))**2)/len(y_cluster2)\n",
    "r_squared = 1 - np.min(mse_values)/ss_tot\n",
    "print(\"Mean coefs\", mean_coef[np.where(alphas == alpha_min)[0][0]])\n",
    "print(\"best alpha:\",alpha_min,\"sse:\",np.min(mse_values), \"\\nss_tot:\",ss_tot, \"\\nr_squared:\",r_squared.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ransac Regressor, with linear regression as estimator and kfold cross validation\n",
    "\n",
    "ransac = RANSACRegressor(estimator = LinearRegression(), residual_threshold=0.7 ,min_samples=15, max_trials=500_000, random_state=42)\n",
    "ransac.fit(x, y)\n",
    "\n",
    "inlier_mask = ransac.inlier_mask_\n",
    "inliers = [i for i, x in enumerate(inlier_mask) if x == True]\n",
    "outliers = [i for i, x in enumerate(inlier_mask) if x == False]\n",
    "print(\"Inliers:\",inliers)\n",
    "print(\"Outliers:\",outliers)\n",
    "\n",
    "x_cluster1, y_cluster1 = x[inliers], y[inliers]\n",
    "x_cluster2, y_cluster2 = x[outliers], y[outliers]\n",
    "\n",
    "print(\"Cluster 1:\",x_cluster1.shape, y_cluster1.shape)\n",
    "print(\"Cluster 2:\",x_cluster2.shape, y_cluster2.shape)\n",
    "\n",
    "# Linear Regression with cross validation (kfold)\n",
    "print(\"===== Model 1 =====\")\n",
    "kf = KFold(n_splits=15, shuffle=True, random_state=47)\n",
    "predicted_values = []\n",
    "sse_values = []\n",
    "coef_values = []\n",
    "for train_index, test_index in kf.split(x_cluster1):\n",
    "    x_train, x_test = x_cluster1[train_index], x_cluster1[test_index]\n",
    "    y_train, y_test = y_cluster1[train_index], y_cluster1[test_index]\n",
    "\n",
    "    lregr = LinearRegression()\n",
    "    lregr.fit(x_train, y_train)\n",
    "    y_pred = lregr.predict(x_test)\n",
    "    sse_values.append(np.mean((y_test - y_pred)**2))\n",
    "\n",
    "    coef_values.append(lregr.coef_)\n",
    "    predicted_values.append(y_pred.flatten())\n",
    "    table = np.column_stack((y_test, y_pred))\n",
    "    \n",
    "sse = np.mean(sse_values)\n",
    "ss_tot = sum((y_cluster1 - np.mean(y_cluster1))**2)/len(y_cluster1)\n",
    "r_squared = 1 - sse/ss_tot\n",
    "\n",
    "mean_coef = np.mean(coef_values, axis=0)\n",
    "print(\"Mean coefs\", mean_coef)\n",
    "print(\"sse:\",sse, \"\\nss_tot:\",ss_tot, \"\\nr_squared:\",r_squared)\n",
    "\n",
    "\n",
    "print(\"===== Model 2 =====\")\n",
    "kf2 = KFold(n_splits=15, shuffle=True, random_state=49)\n",
    "predicted_values = []\n",
    "sse_values = []\n",
    "coef_values = []\n",
    "\n",
    "for train_index, test_index in kf2.split(x_cluster2):\n",
    "    x_train, x_test = x_cluster2[train_index], x_cluster2[test_index]\n",
    "    y_train, y_test = y_cluster2[train_index], y_cluster2[test_index]\n",
    "\n",
    "    lregr = LinearRegression()\n",
    "    lregr.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = lregr.predict(x_test)\n",
    "    sse_values.append(np.mean((y_test - y_pred)**2))\n",
    "\n",
    "    coef_values.append(lregr.coef_)\n",
    "    predicted_values.append(y_pred.flatten())\n",
    "    table = np.column_stack((y_test, y_pred))\n",
    "\n",
    "sse = np.mean(sse_values)\n",
    "ss_tot = sum((y_cluster2 - np.mean(y_cluster2))**2)/len(y_cluster2)\n",
    "r_squared = 1 - sse/ss_tot\n",
    "\n",
    "mean_coef = np.mean(coef_values, axis=0)\n",
    "print(\"Mean coefs\", mean_coef)\n",
    "print(\"sse:\",sse, \"\\nss_tot:\",ss_tot, \"\\nr_squared:\",r_squared)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,4):\n",
    "    counts, bins = np.histogram(x_cluster1[:,i])\n",
    "    plt.hist(bins[:-1], bins, weights=counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,4):\n",
    "    counts, bins = np.histogram(x_cluster2[:,i])\n",
    "    plt.hist(bins[:-1], bins, weights=counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ransac Regressor, with linear regression as estimator and kfold cross validation\n",
    "thresholds = np.linspace(0.5, 1, 100)\n",
    "r_squared_values_m1 = []\n",
    "r_squared_values_m2 = []\n",
    "for threshold in thresholds:\n",
    "    ransac = RANSACRegressor(estimator = LinearRegression(), residual_threshold=threshold,min_samples=15, max_trials=500_000, random_state=47)\n",
    "    ransac.fit(x, y)\n",
    "\n",
    "    inlier_mask = ransac.inlier_mask_\n",
    "    inliers = [i for i, x in enumerate(inlier_mask) if x == True]\n",
    "    outliers = [i for i, x in enumerate(inlier_mask) if x == False]\n",
    "\n",
    "    x_cluster1, y_cluster1 = x[inliers], y[inliers]\n",
    "    x_cluster2, y_cluster2 = x[outliers], y[outliers]\n",
    "\n",
    "    # Linear Regression with cross validation (kfold)\n",
    "    #print(\"===== Model 1 =====\")\n",
    "    kf = KFold(n_splits=len(x_cluster1), shuffle=True, random_state=47)\n",
    "    predicted_values = []\n",
    "    sse_values = []\n",
    "    coef_values = []\n",
    "\n",
    "    for train_index, test_index in kf.split(x_cluster1):\n",
    "        x_train, x_test = x_cluster1[train_index], x_cluster1[test_index]\n",
    "        y_train, y_test = y_cluster1[train_index], y_cluster1[test_index]\n",
    "        lregr = LinearRegression()\n",
    "        lregr.fit(x_train, y_train)\n",
    "\n",
    "        y_pred = lregr.predict(x_test)\n",
    "        sse_values.append(np.mean((y_test - y_pred)**2))\n",
    "\n",
    "        coef_values.append(lregr.coef_)\n",
    "        predicted_values.append(y_pred.flatten())\n",
    "        table = np.column_stack((y_test, y_pred))\n",
    "        \n",
    "    sse = np.mean(sse_values)\n",
    "    ss_tot = sum((y_cluster1 - np.mean(y_cluster1))**2)/len(y_cluster1)\n",
    "    r_squared = 1 - sse/ss_tot\n",
    "    r_squared_values_m1.append(r_squared)\n",
    "\n",
    "    mean_coef = np.mean(coef_values, axis=0)\n",
    "\n",
    "\n",
    "    #print(\"===== Model 2 =====\")\n",
    "    kf2 = KFold(n_splits=len(x_cluster2), shuffle=True, random_state=49)\n",
    "    predicted_values = []\n",
    "    sse_values = []\n",
    "    coef_values = []\n",
    "\n",
    "    for train_index, test_index in kf2.split(x_cluster2):\n",
    "        x_train, x_test = x_cluster2[train_index], x_cluster2[test_index]\n",
    "        y_train, y_test = y_cluster2[train_index], y_cluster2[test_index]\n",
    "\n",
    "        lregr = LinearRegression()\n",
    "        lregr.fit(x_train, y_train)\n",
    "\n",
    "        y_pred = lregr.predict(x_test)\n",
    "        sse_values.append(np.mean((y_test - y_pred)**2))\n",
    "\n",
    "        coef_values.append(lregr.coef_)\n",
    "        predicted_values.append(y_pred.flatten())\n",
    "        table = np.column_stack((y_test, y_pred))\n",
    "\n",
    "    sse = np.mean(sse_values)\n",
    "    ss_tot = sum((y_cluster2 - np.mean(y_cluster2))**2)/len(y_cluster2)\n",
    "    r_squared = 1 - sse/ss_tot\n",
    "    r_squared_values_m2.append(r_squared)\n",
    "\n",
    "    mean_coef = np.mean(coef_values, axis=0)\n",
    "\n",
    "#join in a table with columns: threshold, r_squared_m1, r_squared_m2\n",
    "df = pd.DataFrame({'threshold':thresholds, 'r_squared_m1':r_squared_values_m1, 'r_squared_m2':r_squared_values_m2})\n",
    "print(\"FINAL TABLE\")\n",
    "#best threshold is the one with the highest sum of r_squared_m1 and r_squared_m2\n",
    "df['sum'] = df['r_squared_m1'] + df['r_squared_m2']\n",
    "df = df.sort_values(by=['sum'], ascending=True)\n",
    "\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ransac = RANSACRegressor(estimator = LinearRegression(), min_samples=5, max_trials=500_000, random_state=42)\n",
    "#nota: residual_threshold= x is the maximum distance for a sample to be classified as an inlier\n",
    "#nota: residual_threshold = 1 is the default \n",
    "# mano nao sejas mau, dá la undeafen        #ok, queres que calcule isso? tanto q o default q eles usam é o MAD(median absolute deviation)\n",
    "ransac.fit(x, y)\n",
    "#residual factor used? \n",
    "print(\"Residual factor:\",ransac.residual_threshold)\n",
    "\n",
    "inlier_mask = ransac.inlier_mask_\n",
    "inliers = []\n",
    "for i in range(len(inlier_mask)):\n",
    "    if inlier_mask[i] == True:\n",
    "        inliers.append(i)\n",
    "print(\"Inliers:\",inliers)\n",
    "print(\"Inliers len:\",len(inliers))\n",
    "\n",
    "outliers  = []\n",
    "for i in range(len(inlier_mask)):\n",
    "    if inlier_mask[i] == False:\n",
    "        outliers.append(i)\n",
    "print(\"Outliers:\",outliers)\n",
    "print(\"Outliers len:\",len(outliers))\n",
    "\n",
    "x_inliers = x[inliers]\n",
    "y_inliers = y[inliers]\n",
    "#print(ransac.score(x_inliers, y_inliers)\n",
    "\n",
    "# Get the outlier mask (points that do not fit the best model)\n",
    "outlier_mask = np.logical_not(inlier_mask)\n",
    "\n",
    "# Extract the coefficients of the best model (intercept and slopes for each feature)\n",
    "intercept = ransac.estimator_.intercept_\n",
    "coefficients = ransac.estimator_.coef_\n",
    "\n",
    "print(\"Coefs:\",coefficients.flatten())\n",
    "print(\"Score for inliers:\")\n",
    "print(ransac.score(x_inliers, y_inliers))\n",
    "print(\"Score for outliers:\")\n",
    "print(ransac.score(x[outliers], y[outliers]))\n",
    "\n",
    "model1 = ransac\n",
    "model1_coef = coefficients.flatten()\n",
    "model1_indexes = x_inliers[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_outliers = x[outliers]\n",
    "y_outliers = y[outliers]\n",
    "\n",
    "# make regression line for outliers\n",
    "lin = LinearRegression()\n",
    "lin.fit(x_outliers, y_outliers)\n",
    "y_pred = lin.predict(x_outliers)\n",
    "print(\"Score for outliers with linear regression:\")\n",
    "print(lin.score(x_outliers, y_outliers))\n",
    "\n",
    "\n",
    "#now, trying leave on out cross validation\n",
    "loo = LeaveOneOut()\n",
    "predicted_values = []\n",
    "sse_values = []\n",
    "coef_values = []\n",
    "\n",
    "for train_index, test_index in loo.split(x_outliers):\n",
    "    x_train, x_test = x_outliers[train_index], x_outliers[test_index]\n",
    "    y_train, y_test = y_outliers[train_index], y_outliers[test_index]\n",
    "\n",
    "    lregr = LinearRegression()\n",
    "    lregr.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = lregr.predict(x_test)\n",
    "    sse_values.append(np.mean((y_test - y_pred)**2))\n",
    "\n",
    "    coef_values.append(lregr.coef_)\n",
    "    predicted_values.append(y_pred.flatten())\n",
    "\n",
    "#print list sses for all iterations\n",
    "print(\"SSE values for all iterations:\",sse_values)\n",
    "print(\"max sse:\",max(sse_values))\n",
    "\n",
    "mean_coef = np.mean(coef_values, axis=0)\n",
    "mse = np.mean((y_outliers-predicted_values)**2)\n",
    "var_y = sum((y_outliers - np.mean(y_outliers))**2)/len(y_outliers)\n",
    "r_squared = 1 - mse/var_y\n",
    "\n",
    "print(mean_coef[0], f\"\\nmse is: {mse}\", f\"\\nr^2 is: {r_squared.flatten()}\")\n",
    "\n",
    "model2 = lin\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
